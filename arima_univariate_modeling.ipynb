{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eSFQtGzEtb3E"
   },
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "from pandas import concat\n",
    "from pandas import Series\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import itertools\n",
    "\n",
    "import numpy\n",
    "from numpy import concatenate\n",
    "from numpy import array\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "import math\n",
    "from math import sqrt\n",
    "\n",
    "# ARIMA AND LSTM STUFF\n",
    "from statsmodels.tsa.arima_model import ARIMA\n",
    "from statsmodels.tools.eval_measures import rmse\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "from keras.preprocessing.sequence import TimeseriesGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "\n",
    "import time\n",
    "import statsmodels.api as sm\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5YR3eKzewMi7"
   },
   "source": [
    "# Get data from database tables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "daEcyWjEtzK1",
    "outputId": "bd466db2-63cc-4d88-9ab4-6537a4ce1f57"
   },
   "outputs": [],
   "source": [
    "'''from google.colab import drive\n",
    "drive.mount(\"/content/drive\")\n",
    "path = '/content/drive/My Drive/DS4A-MX7/config_anahuac_copy.json'\n",
    "\n",
    "import json\n",
    "with open(path) as f:\n",
    "    config_file = json.load(f)\n",
    "    \n",
    "# connecting to external database\n",
    "pd.options.display.max_columns = 500\n",
    "connection = psycopg2.connect(user = config_file['user'],\n",
    "                            password = config_file['password'],\n",
    "                            host= config_file['host'],\n",
    "                            port = \"5432\",\n",
    "                            database = config_file['dbname'])\n",
    "\n",
    "'''\n",
    "\n",
    "connection = psycopg2.connect(user = \"equipo7\",\n",
    "                            password = \"DLgndXy2m4hbWH-qn-Co\",\n",
    "                            host= \"ds4a-demo-instance.cssn41frspmj.us-east-1.rds.amazonaws.com\",\n",
    "                            port = \"5432\",\n",
    "                            database = \"anahuac\")\n",
    "\n",
    "\n",
    "\n",
    "cursor = connection.cursor()\n",
    "def runquery(query):\n",
    "    df=pd.read_sql(query,connection)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VPw1whY9uRny"
   },
   "outputs": [],
   "source": [
    "def pivot_by_factor(df, index_table, column_table, value_table):\n",
    "    df = pd.pivot_table(df, index = index_table, columns = column_table, values = value_table)\n",
    "    df = df.sort_index(axis=1, level=1)\n",
    "    df.columns = [f'{b}_{a}' for a, b in df.columns]\n",
    "    df = df.reset_index()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iOXnCY-IutaQ"
   },
   "outputs": [],
   "source": [
    "where_date_clause = \" WHERE date >= '2012-01-01' AND date <= '2020-03-01 00:00:00' ORDER BY date\"; \n",
    "\n",
    "sqlQuery_radiacion = \"select * from processed_metrics.radiacion \" + where_date_clause\n",
    "df_radiacion = pd.DataFrame(runquery(sqlQuery_radiacion))\n",
    "\n",
    "sqlQuery_rama = \"select * from processed_metrics.rama \" + where_date_clause\n",
    "df_rama = pd.DataFrame(runquery(sqlQuery_rama))\n",
    "\n",
    "sqlQuery_redmet = \"select * from processed_metrics.redmet \" + where_date_clause\n",
    "df_redmet = pd.DataFrame(runquery(sqlQuery_redmet))\n",
    "\n",
    "sqlQuery_presion = \"select * from processed_metrics.presion \" + where_date_clause\n",
    "df_presion = pd.DataFrame(runquery(sqlQuery_presion))\n",
    "\n",
    "sqlQuery_weather = \"select date,fecha,hora,dewp_c AS dew_point_st,rh_st AS rain_st from processed_metrics.weather_fix  \" + where_date_clause\n",
    "df_weather = pd.DataFrame(runquery(sqlQuery_weather))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aDXCZQIwu1l6"
   },
   "outputs": [],
   "source": [
    "#radiation only has data until 2019-12-31 23:00:00. pressure has one more month but we will not consider it\n",
    "#process radiation\n",
    "df_radiacion = pivot_by_factor(df_radiacion,[\"date\",\"fecha\",\"hora\"],[\"factor\"],[\"st\"])\n",
    "\n",
    "#process pa\n",
    "df_presion.rename(columns={'st': 'PA_st'}, inplace=True)\n",
    "del df_presion['factor']\n",
    "\n",
    "#process pollutants\n",
    "df_rama = pivot_by_factor(df_rama,[\"date\",\"fecha\",\"hora\"],[\"factor\"],[\"st0\",\"st1\",\"st2\"])\n",
    "\n",
    "#process redmet\n",
    "df_redmet = pivot_by_factor(df_redmet,[\"date\",\"fecha\",\"hora\"],[\"factor\"],[\"st0\",\"st1\",\"st2\"])\n",
    "\n",
    "#weather, we are interested in rain and dewpoint, already cleaned when reading from DB\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tw3PtQyDu7sK"
   },
   "outputs": [],
   "source": [
    "#merge all data as one\n",
    "from functools import reduce\n",
    "dfs = [df_rama, df_redmet, df_presion, df_radiacion, df_weather]\n",
    "df_workspace = reduce(lambda left,right: pd.merge(left,right,on=['date','fecha','hora'], how='left'), dfs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5hk_VujOwGX0"
   },
   "source": [
    "# Final dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 320
    },
    "colab_type": "code",
    "id": "IuGcqczvu_wi",
    "outputId": "bcc8b709-f467-4add-a104-8436b49f7fe4"
   },
   "outputs": [],
   "source": [
    "df_workspace.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 181
    },
    "colab_type": "code",
    "id": "-GYXvxlAvCXR",
    "outputId": "df49b56f-9530-4393-e43e-a75dbe7d71c4"
   },
   "outputs": [],
   "source": [
    "df_workspace.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "54DgvuH3vGCD"
   },
   "outputs": [],
   "source": [
    "def plot_resample_date(df,start_date,end_date, cols, resample_by):\n",
    "    df[start_date:end_date][cols].resample(resample_by).mean().plot(figsize=(15, 6))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "f1lr-Eh-vHrg"
   },
   "outputs": [],
   "source": [
    "# set date as index\n",
    "df_workspace.set_index('date', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_BB2DqlSwV_p"
   },
   "source": [
    "# Review features by different resampling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "AztZLGvLvKKl",
    "outputId": "1362ff32-d99d-403d-e972-83a66bfdeb71"
   },
   "outputs": [],
   "source": [
    "start_year = str(2012)\n",
    "end_year   = str(2020)\n",
    "\n",
    "for s in ['W', 'MS', 'Q', 'Y']:\n",
    "    print(\"SAMPLE BY: \", s)\n",
    "    resample_by = s\n",
    "    plot_resample_date(df_workspace,start_year,end_year ,[ 'PA_st'], resample_by )\n",
    "    plot_resample_date(df_workspace,start_year,end_year ,[ 'UVA_st','UVB_st'], resample_by)\n",
    "    plot_resample_date(df_workspace,start_year,end_year ,[ 'CO_st0','CO_st1', 'CO_st2'], resample_by)\n",
    "    plot_resample_date(df_workspace,start_year,end_year ,[ 'NO_st0', 'NO_st1', 'NO_st2'], resample_by)\n",
    "    plot_resample_date(df_workspace,start_year,end_year ,[ 'NO2_st0', 'NO2_st1','NO2_st2'], resample_by)\n",
    "    plot_resample_date(df_workspace,start_year,end_year ,[ 'O3_st0', 'O3_st1','O3_st2'], resample_by)\n",
    "    plot_resample_date(df_workspace,start_year,end_year ,[ 'PM10_st0', 'PM10_st1', 'PM10_st2'], resample_by)\n",
    "    plot_resample_date(df_workspace,start_year,end_year ,[ 'PM25_st0', 'PM25_st1', 'PM25_st2'], resample_by)\n",
    "    plot_resample_date(df_workspace,start_year,end_year ,[ 'PMCO_st0', 'PMCO_st1', 'PMCO_st2'], resample_by)\n",
    "    plot_resample_date(df_workspace,start_year,end_year ,[ 'SO2_st0', 'SO2_st1','SO2_st2'], resample_by)\n",
    "    plot_resample_date(df_workspace,start_year,end_year ,[ 'RH_st0', 'RH_st1', 'RH_st2'], resample_by)\n",
    "    plot_resample_date(df_workspace,start_year,end_year ,[ 'TMP_st0', 'TMP_st1','TMP_st2'], resample_by)\n",
    "    plot_resample_date(df_workspace,start_year,end_year ,[ 'WDR_st0', 'WDR_st1', 'WDR_st2'], resample_by)\n",
    "    plot_resample_date(df_workspace,start_year,end_year ,[ 'WSP_st0', 'WSP_st1', 'WSP_st2'], resample_by)\n",
    "    plot_resample_date(df_workspace,start_year,end_year ,[ 'dew_point_st'], resample_by)\n",
    "    plot_resample_date(df_workspace,start_year,end_year ,[ 'rain_st'], resample_by)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forecasting with ARIMA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time series provide the opportunity to forecast future values. Based on previous values, time series can be used to forecast trends in economics, weather, and capacity planning, to name a few. The specific properties of time-series data mean that specialized statistical methods are usually required. In this case pollution features will be predicted.\n",
    "\n",
    "There are three distinct integers (p, d, q) that are used to parametrize ARIMA models. Because of that, ARIMA models are denoted with the notation ARIMA(p, d, q). Together these three parameters account for seasonality, trend, and noise in datasets:\n",
    "\n",
    "    p is the auto-regressive part of the model. It allows us to incorporate the effect of past values into our model. Intuitively, this would be similar to stating that it is likely to be warm tomorrow if it has been warm the past 3 days.\n",
    "    d is the integrated part of the model. This includes terms in the model that incorporate the amount of differencing (i.e. the number of past time points to subtract from the current value) to apply to the time series. Intuitively, this would be similar to stating that it is likely to be same temperature tomorrow if the difference in temperature in the last three days has been very small.\n",
    "    q is the moving average part of the model. This allows us to set the error of our model as a linear combination of the error values observed at previous time points in the past.\n",
    "\n",
    "When dealing with seasonal effects, we make use of the seasonal ARIMA, which is denoted as ARIMA(p,d,q)(P,D,Q)s. Here, (p, d, q) are the non-seasonal parameters described above, while (P, D, Q) follow the same definition but are applied to the seasonal component of the time series. The term s is the periodicity of the time series (4 for quarterly periods, 12 for yearly periods, etc.).\n",
    "\n",
    "The seasonal ARIMA method can appear daunting because of the multiple tuning parameters involved. In the next section, we will describe how to automate the process of identifying the optimal set of parameters for the seasonal ARIMA time series model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yhn-AytmwdYB"
   },
   "source": [
    "# Analizing parameters for ARIMA PM10 and CO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "There are several features, we focus in only two of them PM10 and CO2. These were selected because they\n",
    "show different behavior over time. PM10 varies and is very errant likewise PM2.5 and O3.\n",
    "In the other hand CO shows a repetitive pattern that ww believe ARIMA will give a result that we can use as a baseline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "R2g_d27Gwlfg",
    "outputId": "cfb20a47-88d3-43df-a81c-cac33f152247"
   },
   "outputs": [],
   "source": [
    "from pandas.plotting import autocorrelation_plot\n",
    "from statsmodels.graphics.tsaplots import plot_pacf\n",
    "\n",
    "#Check autocorrelation to have an idea of parameters\n",
    "for s in ['H','D','W', 'MS', 'Q', 'Y']:\n",
    "    print(\"SAMPLE BY: \", s)\n",
    "    resampled = df_workspace['PM10_st0'].resample(s).mean()\n",
    "    autocorrelation_plot(resampled)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "lMpj0F7dx5Jg",
    "outputId": "90aed7ef-e4a6-4094-8e2b-8f4242a7c5cb"
   },
   "outputs": [],
   "source": [
    "for s in ['H','D','W', 'MS', 'Q', 'Y']:\n",
    "    print(\"SAMPLE BY: \", s)\n",
    "    resampled = df_workspace['PM10_st0'].resample(s).mean()\n",
    "    plot_pacf(resampled, lags=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sYspb7vswyhR"
   },
   "outputs": [],
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "F7YrORWIyUlQ"
   },
   "source": [
    "# Create train test dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "colab_type": "code",
    "id": "AywneUNEyZlp",
    "outputId": "544a099c-1d1f-43e9-c6d9-3426aadd51f8"
   },
   "outputs": [],
   "source": [
    "train_arima      = df_workspace['2017-01-01':'2018-12-31']\n",
    "test_arima       = df_workspace['2019-09-01':'2019-12-31']\n",
    "validation_arima = df_workspace['2020-01-01':'2020-02-29']\n",
    "\n",
    "train_hours      = len(train_arima)\n",
    "test_hours       = len(test_arima)\n",
    "validation_hours = len(train_arima)\n",
    "\n",
    "\n",
    "print(train_arima.shape)\n",
    "print(test_arima.shape)\n",
    "print(validation_arima.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ac5p18KxydPP"
   },
   "source": [
    "# Parameter Selection for the ARIMA Time Series Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We used 2 years of training (2017-01-01 to 2018-12-31) and one year of training (2019-09-01 to 2019-12-31).\n",
    "\n",
    "WWe then evaluated train dataset and compared predicted values against the true values of year 2019 \n",
    "(2557 total hours in 2019)  year using brute force (try some of all possible combinations of parameters) \n",
    "to get hyperparameters for both features. We defined the trend string to be 'n' meaning that ot has NO trend.\n",
    "\n",
    "Result of hyperparameter selection and the RMSE for year 2019 is:\n",
    " \n",
    " \n",
    " \n",
    "PM10 for station 1 \n",
    " > Model[[(0, 0, 0), (1, 0, 0, 0), 'n']] with RMSE  9.394 \n",
    " > Model[[(0, 0, 0), (1, 0, 1, 0), 'n']] with RMSE  9.414\n",
    " > Model[[(0, 0, 0), (1, 0, 2, 0), 'n']] with RMSE  9.413\n",
    " > Model[[(0, 0, 0), (2, 0, 0, 0), 'n']] with RMSE  9.414\n",
    " > Model[[(1, 0, 0), (1, 0, 0, 0), 'n']] with RMSE 10.574\n",
    " > Model[[(0, 1, 0), (1, 0, 0, 0), 'n']] with RMSE 10.897\n",
    " > Model[[(0, 1, 0), (0, 0, 0, 0), 'n']] with RMSE 10.897\n",
    " > Model[[(1, 1, 0), (0, 0, 0, 0), 'n']] with RMSE 10.897\n",
    " > Model[[(0, 0, 0), (0, 0, 2, 0), 'n']] with RMSE 17.313\n",
    " > Model[[(0, 0, 0), (0, 0, 1, 0), 'n']] with RMSE 23.729\n",
    " < Model[[(0, 0, 1), (0, 0, 0, 0), 'n']] with RMSE 23.729\n",
    " > Model[[(0, 0, 0), (0, 0, 0, 0), 'n']] with RMSE 40.769\n",
    "\n",
    "CO for station 1\n",
    " > Model[[(0, 0, 0), (1, 0, 2, 0), 'n']] with RMSE 0.774\n",
    " > Model[[(0, 0, 0), (1, 0, 0, 0), 'n']] with RMSE 0.777\n",
    " > Model[[(0, 0, 0), (1, 0, 1, 0), 'n']] with RMSE 0.777 \n",
    " > Model[[(0, 0, 0), (2, 0, 0, 0), 'n']] with RMSE 0.777\n",
    " > Model[[(0, 1, 0), (1, 0, 0, 0), 'n']] with RMSE 0.787\n",
    " > Model[[(0, 1, 0), (0, 0, 0, 0), 'n']] with RMSE 0.787\n",
    " > Model[[(1, 1, 0), (0, 0, 0, 0), 'n']] with RMSE 0.787\n",
    " > Model[[(1, 0, 0), (1, 0, 0, 0), 'n']] with RMSE 0.851\n",
    " > Model[[(0, 0, 0), (0, 0, 2, 0), 'n']] with RMSE 1.164\n",
    " > Model[[(0, 0, 1), (0, 0, 0, 0), 'n']] with RMSE 1.475\n",
    " > Model[[(0, 0, 0), (0, 0, 1, 0), 'n']] with RMSE 1.475\n",
    " > Model[[(0, 0, 0), (0, 0, 0, 0), 'n']] with RMSE 2.404\n",
    "\n",
    " \n",
    "\n",
    " \n",
    "   \n",
    " \n",
    " \n",
    " As expected PM10 has worst RMSE 40.769  compared to CO\n",
    " \n",
    " We decide using **Model[[(0, 0, 0), (1, 0, 0, 0), 'n']]** for all features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "upji5o53y1-f",
    "outputId": "00ad2a57-7a53-470a-9ed5-732175af6bca"
   },
   "outputs": [],
   "source": [
    "pip install funcy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 90
    },
    "colab_type": "code",
    "id": "46tQtknby5GM",
    "outputId": "21c2a0ff-0c56-4311-c98d-33e66979bae2"
   },
   "outputs": [],
   "source": [
    "pip install numba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bVI1l8NXvr5x"
   },
   "outputs": [],
   "source": [
    "# grid search sarima hyperparameters\n",
    "from math import sqrt\n",
    "from multiprocessing import cpu_count\n",
    "from joblib import Parallel\n",
    "from joblib import delayed\n",
    "from warnings import catch_warnings\n",
    "from warnings import filterwarnings\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# one-step sarima forecast\n",
    "def sarima_forecast(history, config):\n",
    "    order, sorder, trend = config\n",
    "    # define model\n",
    "    model = SARIMAX(history, order=order, seasonal_order=sorder, trend=trend, enforce_stationarity=False, enforce_invertibility=False)\n",
    "    # fit model\n",
    "    model_fit = model.fit(disp=False)\n",
    "    # make one step forecast\n",
    "    yhat = model_fit.predict(len(history), len(history))\n",
    "    return yhat[0]\n",
    "\n",
    "# root mean squared error or rmse\n",
    "def measure_rmse(actual, predicted):\n",
    "    return sqrt(mean_squared_error(actual, predicted))\n",
    "\n",
    "# walk-forward validation for univariate data\n",
    "def walk_forward_validation(train, test, cfg):\n",
    "    predictions = list()\n",
    "    # seed history with training dataset\n",
    "    history = [x for x in train]\n",
    "    # step over each time-step in the test set\n",
    "    for i in range(len(test)):\n",
    "        # fit model and make forecast for history\n",
    "        yhat = sarima_forecast(history, cfg)\n",
    "        # store forecast in list of predictions\n",
    "        predictions.append(yhat)\n",
    "        # add actual observation to history for the next loop\n",
    "        history.append(test[i])\n",
    "    # estimate prediction error\n",
    "    error = measure_rmse(test, predictions)\n",
    "    return error\n",
    "\n",
    "# score a model, return None on failure\n",
    "def score_model(train, test, cfg, debug=False):\n",
    "    result = None\n",
    "    # convert config to a key\n",
    "    key = str(cfg)\n",
    "    # show all warnings and fail on exception if debugging\n",
    "    # one failure during model validation suggests an unstable config\n",
    "    try:\n",
    "        # never show warnings when grid searching, too noisy\n",
    "        with catch_warnings():\n",
    "            filterwarnings(\"ignore\")\n",
    "            result = walk_forward_validation(train, test, cfg)\n",
    "    except:\n",
    "        error = None\n",
    "    # check for an interesting result\n",
    "    if result is not None:\n",
    "        print(' > Model[%s] %.3f' % (key, result))\n",
    "    return (key, result)\n",
    "\n",
    "# grid search configs\n",
    "def grid_search(train, test, cfg_list):\n",
    "    scores = None\n",
    "    # execute configs in parallel\n",
    "    executor = Parallel(n_jobs=cpu_count(),  backend='multiprocessing')\n",
    "    tasks = (delayed(score_model)(train, test, cfg) for cfg in cfg_list)\n",
    "    scores = executor(tasks)\n",
    "    # remove empty results\n",
    "    scores = [r for r in scores if r[1] != None]\n",
    "    # sort configs by error, asc\n",
    "    scores.sort(key=lambda tup: tup[1])\n",
    "    return scores\n",
    "\n",
    "\n",
    "def sarima_configs(seasonal=[0]):\n",
    "    models = list()\n",
    "    models.append([(1, 0, 0), (1, 0, 0, 0), 'n'])\n",
    "    models.append([(1, 0, 0), (1, 0, 1, 0), 'n'])\n",
    "    models.append([(1, 0, 0), (1, 0, 2, 0), 'n'])\n",
    "    models.append([(2, 0, 0), (1, 0, 0, 0), 'n'])\n",
    "    models.append([(2, 0, 0), (1, 0, 1, 0), 'n'])\n",
    "    models.append([(2, 0, 0), (1, 0, 2, 0), 'n'])\n",
    "    \n",
    "    return models\n",
    "\n",
    "def sarima_configs(seasonal=[0]):\n",
    "    models = list()\n",
    "    # define config lists\n",
    "    p_params = [0, 1, 2]\n",
    "    d_params = [0, 1]\n",
    "    q_params = [0, 1, 2]\n",
    "    t_params = ['n'] # there are no trend 'c','t','ct'\n",
    "    P_params = [0, 1, 2]\n",
    "    D_params = [0, 1]\n",
    "    Q_params = [0, 1, 2]\n",
    "    m_params = seasonal\n",
    "    # create config instances\n",
    "    for p in p_params:\n",
    "        for d in d_params:\n",
    "            for q in q_params:\n",
    "                for t in t_params:\n",
    "                    for P in P_params:\n",
    "                        for D in D_params:\n",
    "                            for Q in Q_params:\n",
    "                                for m in m_params:\n",
    "                                    cfg = [(p,d,q), (P,D,Q,m), t]\n",
    "                                    models.append(cfg)\n",
    "    return models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "8LjBfVdEnoTM",
    "outputId": "fc2c9d68-0f79-42de-b379-13c0e9907e2c"
   },
   "outputs": [],
   "source": [
    "cpus = cpu_count()\n",
    "print(\"available processors: \", cpus )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "2BhFe_pOy-qm",
    "outputId": "350caed1-c7ef-4dea-c432-162f25cd8df0"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# model configs\n",
    "cfg_list = sarima_configs()\n",
    "#Carbon monoxide , Nitrogen Dioxide, Sulfur dioxide\n",
    "arima_models = ['PM10_st0','CO_st0']#'NO2_st0', 'SO2_st0']\n",
    "\n",
    "for m in arima_models:\n",
    "    # grid search\n",
    "    print('Grid search for: ', m)\n",
    "    scores = grid_search(train_arima[m],test_arima[m], cfg_list )\n",
    "    # list top 3 configs\n",
    "    for cfg, error in scores[:3]:\n",
    "        print(cfg, error)  \n",
    "        print('done for: ', m) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DEQ9MTxibt3N"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Untitled0.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
