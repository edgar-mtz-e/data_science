{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import requests\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy\n",
    "import openpyxl\n",
    "from bs4 import BeautifulSoup\n",
    "from pathlib import Path\n",
    "from openpyxl.utils.dataframe import dataframe_to_rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetching data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A script was created to download all datasets, unzip and put them in their corresponding ```data``` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile, urllib.request, shutil\n",
    "import time\n",
    "from pathlib import Path\n",
    "from urllib.parse import urlparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "cwd = Path.cwd()\n",
    "\n",
    "\n",
    "def CreateDir(path):\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "\n",
    "#downloads zip file from an url and extracts it to a folder\n",
    "def DownloadDatasets(url, file_name, dir_name):\n",
    "    try:\n",
    "        with urllib.request.urlopen(url) as response, open(file_name, 'wb') as out_file:\n",
    "            shutil.copyfileobj(response, out_file)        \n",
    "            with zipfile.ZipFile(file_name,mode='r') as zf:\n",
    "                zf.extractall(dir_name)\n",
    "        time.sleep(3)\n",
    "        #move zip to dir_name\n",
    "        shutil.move(file_name, dir_name)\n",
    "    except Exception:\n",
    "        print (\"Something happened with zip file \",file_name)\n",
    "        pass     \n",
    "\n",
    "def DownloadFrom(base_url, area, start_year, end_year):\n",
    "    UrlsByYear = [str(i) for i in range(start_year,end_year)]\n",
    "    UrlsByYear = [s[2:]  for s in UrlsByYear]\n",
    "    UrlsByYear = [base_url + s + area + \".zip\" for s in UrlsByYear] \n",
    "    for url in UrlsByYear:\n",
    "        print(\"Getting data from: \", url)\n",
    "        infoUrl = urlparse(url)\n",
    "        file_name = os.path.basename(infoUrl.path)\n",
    "        dir_name = str(cwd) + \"/\" + area + \"/\" + str(os.path.splitext(file_name)[0])\n",
    "        CreateDir(dir_name)\n",
    "        DownloadDatasets(url, file_name, dir_name)\n",
    "        \n",
    "\n",
    "#DownloadFrom(\"http://datosabiertos.aire.cdmx.gob.mx:8080/opendata/excel/RAMA/\",\"RAMA\",1986, 2021)\n",
    "#DownloadFrom(\"http://datosabiertos.aire.cdmx.gob.mx:8080/opendata/excel/REDMET/\",\"REDMET\",1986,2021)\n",
    "#DownloadFrom(\"http://datosabiertos.aire.cdmx.gob.mx:8080/opendata/excel/REDMA/\",\"REDMA\",1986,2021)\n",
    "#DownloadFrom(\"http://datosabiertos.aire.cdmx.gob.mx:8080/opendata/excel/REDDA/\",\"REDDA\",1988,2020)\n",
    "#DownloadFrom(\"http://datosabiertos.aire.cdmx.gob.mx:8080/opendata/excel/RADIACION/\",\"RADIACION\",2000,2021)  #EL AÃ‘O 2020 NO EXISTE EN LA PAG\n",
    "#DownloadFrom(\"http://datosabiertos.aire.cdmx.gob.mx:8080/opendata/excel/PRESION/\",\"PRESION\",2009,2021)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset consolidation:\n",
    "\n",
    "To have a consolidated dataset for each category we propose the following method:\n",
    "1. For each category folder: **PRESION, RADIACION, RAMA, REDDA, REDMA and REDMET** we loop all its directories (organized by year in the range ```1986-2020``` and containing Factor data. Read those files that have extension ```.xls```.\n",
    "2. For each ```excel factor file``` having data for Factors ```CO2, NO2, NOX, 03, PM10, SO2, UVM, etc.```. Load a dataframe from excel data, then add corresponding  ```Factor``` label column to characterize the data. \n",
    "3. Loop all dataframes to get all columns that must be present in **condolidated** dataset. \n",
    "4. Check if current dataframe has all general columns, if that is not the case we add a column with null values repreented by ```NaNfile``` **-99**. \n",
    "5. Merge all dataframes and save it to a new file. Result of this procedure is a ```single dataset per year```.\n",
    "6. Finally we repeat the proces using all generated year files to create a single dataset per category.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/sampleYearResult.png\" >\n",
    "<img src=\"img/SampleConsolidated.png\" >\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "cwd = os.getcwd()\n",
    "datad = cwd + '\\\\data\\\\aire.cdmx\\\\'\n",
    "nullValue = -99\n",
    "slash = '\\\\'\n",
    "\n",
    "#returns a set of all columns that must be in all files            \n",
    "def GetAllColumnsFromFiles(yearDir): \n",
    "    Columns = set()\n",
    "    for FactorFile in os.listdir(yearDir):\n",
    "        if FactorFile.endswith('.xls') or FactorFile.endswith('.xlsx'):\n",
    "            xl = pd.ExcelFile(yearDir+FactorFile)            \n",
    "            df = xl.parse(xl.sheet_names[0])\n",
    "            df.columns = df.columns.str.upper()\n",
    "            for c in df.columns:\n",
    "                Columns.add(c.upper())\n",
    "            #print(\"File columns: \", df.columns)\n",
    "            #print(\"number of columns for file: \",len(df.columns))\n",
    "    #print(\"All columns for directory: \", Columns)\n",
    "    #print(\"Total columns for year dataset to use:\", len(Columns))\n",
    "    return list(Columns) \n",
    "\n",
    "#adds a column with a certain value to a dataframe\n",
    "def AddColumnToDataFrame(df, columnName, value):\n",
    "    df[columnName] = value\n",
    "\n",
    "#removes starting year and extension: radiacion_2000UVA.xls,  1986O2.xls etc\n",
    "def GetFactorFromFileName(s):\n",
    "    i = 0\n",
    "    s = s.replace(\"_\",\"\")\n",
    "    for c in s:\n",
    "        if c.isalpha():\n",
    "            i += 1\n",
    "        else:\n",
    "            break\n",
    "    return s[4+i:-4]\n",
    "\n",
    "#Adds missing column and fill it with NaN, then adds a Factor label column \n",
    "def ManageFactorColumnsForDataFrame(temp_df,Columns,FactorFile, addFactorColumn):\n",
    "    for c in Columns:\n",
    "        if c.upper() not in temp_df.columns:\n",
    "            AddColumnToDataFrame(temp_df,c, nullValue)  \n",
    "    if addFactorColumn:        \n",
    "        AddColumnToDataFrame(temp_df, \"FACTOR\", GetFactorFromFileName(FactorFile) ) \n",
    "\n",
    "    \n",
    "def SaveCsv(df, path, csvFileName):\n",
    "    p = os.path.join( str(Path(path).parents[0]) + slash + csvFileName ) + \".csv\"\n",
    "    #print(\"saving to: \", p )\n",
    "    df.to_csv( p, sep=',', encoding='utf-8', header='true', index=False)\n",
    "\n",
    "    \n",
    "def SaveXls(df, path, xlsFileName, compress=False):\n",
    "    p = os.path.join( str(Path(path).parents[0]) + slash + xlsFileName ) + \".xlsx\"\n",
    "    #print(\"saving to: \", p )\n",
    "   \n",
    "    writer = pd.ExcelWriter(p, engine='xlsxwriter')\n",
    "    if compress:\n",
    "        writer.book.use_zip64()\n",
    "    df.to_excel(writer, sheet_name=xlsFileName, index=False)\n",
    "    writer.save()\n",
    "    \n",
    "    '''wb = openpyxl.Workbook()\n",
    "    ws = wb.active\n",
    "    for r in dataframe_to_rows(df, index=False, header=True):\n",
    "        if len(r) > 1:\n",
    "            ws.append(r)\n",
    "    wb.save(p)  \n",
    "    '''\n",
    "    \n",
    "def ConsolidateExcelFiles(workspaceDir, resultFileName, addFactorColumn =True, resultIsXls = True):\n",
    "    #print(\"Starting directory: \", workspaceDir)\n",
    "    dfs = []\n",
    "    Columns = GetAllColumnsFromFiles(workspaceDir)\n",
    "       \n",
    "    for excelFile in os.listdir(workspaceDir):\n",
    "        if excelFile.endswith('.xls') or excelFile.endswith('.xlsx'):\n",
    "            xl = pd.ExcelFile(workspaceDir+excelFile)            \n",
    "            temp_df = xl.parse(xl.sheet_names[0])\n",
    "            temp_df.columns = temp_df.columns.str.upper()\n",
    "            if not temp_df.empty:\n",
    "                ManageFactorColumnsForDataFrame(temp_df, Columns, excelFile,addFactorColumn)\n",
    "                dfs.append(temp_df)\n",
    "            else:\n",
    "                print(\"There is an issue with file: \", excelFile)\n",
    "            print(\"Finished parsing file:\", excelFile) \n",
    "    \n",
    "    merged_df = pd.concat(dfs)\n",
    "    \n",
    "    merged_df[\"FECHA\"] = pd.to_datetime(merged_df[\"FECHA\"])\n",
    "    \n",
    "    #merged_df = merged_df.sort_values(['FECHA', 'HORA', 'FACTOR'], ascending=[True, True])    \n",
    "    if resultIsXls :\n",
    "        #compress = True if df.axes[0] > 1000000 else False\n",
    "        if len(merged_df.axes[0]) < 1000000:\n",
    "            SaveXls(merged_df, workspaceDir, resultFileName ) \n",
    "        else: \n",
    "            print(\"Dataser is to large for excel, saving as csv, no. rows:\", len(merged_df.axes[0]))\n",
    "            SaveCsv(merged_df, workspaceDir, resultFileName)    \n",
    "    else:\n",
    "        SaveCsv(merged_df, workspaceDir, resultFileName) \n",
    "    print(\"---------------------------Finished consolidating file:\", resultFileName)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished parsing file: 00RAMA.xlsx\n",
      "Finished parsing file: 01RAMA.xlsx\n",
      "Finished parsing file: 02RAMA.xlsx\n",
      "Finished parsing file: 03RAMA.xlsx\n",
      "Finished parsing file: 04RAMA.xlsx\n",
      "Finished parsing file: 05RAMA.xlsx\n",
      "Finished parsing file: 06RAMA.xlsx\n",
      "Finished parsing file: 07RAMA.xlsx\n",
      "Finished parsing file: 08RAMA.xlsx\n",
      "Finished parsing file: 09RAMA.xlsx\n",
      "Finished parsing file: 10RAMA.xlsx\n",
      "Finished parsing file: 11RAMA.xlsx\n",
      "Finished parsing file: 12RAMA.xlsx\n",
      "Finished parsing file: 13RAMA.xlsx\n",
      "Finished parsing file: 14RAMA.xlsx\n",
      "Finished parsing file: 15RAMA.xlsx\n",
      "Finished parsing file: 16RAMA.xlsx\n",
      "Finished parsing file: 17RAMA.xlsx\n",
      "Finished parsing file: 18RAMA.xlsx\n",
      "Finished parsing file: 19RAMA.xlsx\n",
      "Finished parsing file: 20RAMA.xlsx\n",
      "Finished parsing file: 86RAMA.xlsx\n",
      "Finished parsing file: 87RAMA.xlsx\n",
      "Finished parsing file: 88RAMA.xlsx\n",
      "Finished parsing file: 89RAMA.xlsx\n",
      "Finished parsing file: 90RAMA.xlsx\n",
      "Finished parsing file: 91RAMA.xlsx\n",
      "Finished parsing file: 92RAMA.xlsx\n",
      "Finished parsing file: 93RAMA.xlsx\n",
      "Finished parsing file: 94RAMA.xlsx\n",
      "Finished parsing file: 95RAMA.xlsx\n",
      "Finished parsing file: 96RAMA.xlsx\n",
      "Finished parsing file: 97RAMA.xlsx\n",
      "Finished parsing file: 98RAMA.xlsx\n",
      "Finished parsing file: 99RAMA.xlsx\n",
      "Dataser is to large for excel, saving as csv, no. rows: 2099184\n",
      "---------------------------Finished consolidating file: RAMA\n"
     ]
    }
   ],
   "source": [
    "def ConsolidateByYear(dirToConsolidate):\n",
    "    for yearFile in os.listdir(dirToConsolidate):\n",
    "        path = dirToConsolidate + yearFile + slash\n",
    "        if os.path.isdir(path): \n",
    "            ConsolidateExcelFiles(path,yearFile, True, True)\n",
    "#main\n",
    "'''\n",
    "for file in os.listdir(datad):\n",
    "    if os.path.isdir(datad+file): \n",
    "        path = os.path.join(datad+file+slash)\n",
    "        ConsolidateByYear(path)\n",
    "'''           \n",
    "\n",
    "workDir =\"C:\\\\Users\\\\Edgar\\\\Desktop\\\\DS4A\\\\Project\\\\data\\\\aire.cdmx\\\\RAMA\\\\\"\n",
    "#ConsolidateByYear(workDir)\n",
    "ConsolidateExcelFiles(workDir, \"RAMA\",  False, True )\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
